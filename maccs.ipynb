{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv \n",
    "import matplotlib.pyplot as plt\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit.Chem import rdMolDescriptors\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "import xgboost as xgb\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.metrics import make_scorer, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import LeaveOneOut, cross_val_score\n",
    "from sklearn.metrics import make_scorer, mean_squared_error, explained_variance_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(filepath):\n",
    "    df = pd.read_excel(filepath)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_maccs_fingerprint(smiles):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    \n",
    "    if mol is None:\n",
    "        print(f\"Invalid SMILES: {smiles}\")\n",
    "        return pd.Series([0] * 166)  # MACCS fingerprints have 166 bits\n",
    "\n",
    "    # Calculate MACCS fingerprint\n",
    "    fingerprint = rdMolDescriptors.GetMACCSKeysFingerprint(mol)\n",
    "    bit_array = list(fingerprint.ToBitString())\n",
    "    \n",
    "\n",
    "    return pd.Series([int(bit) for bit in bit_array])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(df, smiles_column, training_column, target_column):\n",
    "    \n",
    "    fingerprints = df[smiles_column].apply(lambda x: calculate_maccs_fingerprint(x))\n",
    "    \n",
    "    # Combine the original DataFrame with the fingerprints DataFrame\n",
    "    df = pd.concat([df, fingerprints], axis=1)\n",
    "    \n",
    "    # Drop unnecessary columns\n",
    "    df = df.drop(columns=[smiles_column, training_column])\n",
    "    \n",
    "    # Prepare the feature matrix X and target vector y\n",
    "    X = df.drop(columns=[target_column])  \n",
    "    y = df[target_column]                 \n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split into training and test set\n",
    "def split_data(X, Y, test_size=0.2, random_state=42):\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=test_size, random_state=random_state)\n",
    "    X_train.columns = X_train.columns.astype(str)\n",
    "    X_test.columns = X_test.columns.astype(str)\n",
    "    return X_train, X_test, Y_train, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standard Scaler\n",
    "def scale_data(X_train, X_test):\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    return X_train_scaled, X_test_scaled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_models(X_train, X_test, Y_train, Y_test):\n",
    "    models = {\n",
    "        'Linear Regression': LinearRegression(),\n",
    "        'Ridge Regression': Ridge(alpha=1.0, solver='auto'),\n",
    "        'Lasso Regression': Lasso(alpha=0.01, max_iter=1000),\n",
    "        'Elastic Net': ElasticNet(alpha=0.01, l1_ratio=0.5, max_iter=1000),\n",
    "        'Random Forest': RandomForestRegressor(n_estimators=1000, max_depth=10, min_samples_split=2, random_state=42),\n",
    "        'SVR': SVR(kernel='rbf', C=1.0, epsilon=0.1),\n",
    "        'K-Neighbors Regressor': KNeighborsRegressor(n_neighbors=5, weights='uniform', algorithm='auto'),\n",
    "        'XGBoost': xgb.XGBRegressor(n_estimators=100, learning_rate=0.1, max_depth=6, subsample=0.8, colsample_bytree=0.8, random_state=42),\n",
    "        'CatBoost': CatBoostRegressor(learning_rate=0.1, iterations=1000, depth=6, silent=True),\n",
    "        'GradientBoost': GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, subsample=0.8, random_state=42)\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    for name, model in models.items():\n",
    "        model.fit(X_train, Y_train)\n",
    "        Y_pred = model.predict(X_test)\n",
    "        \n",
    "        # Calculate R2 and MSE\n",
    "        r2 = r2_score(Y_test, Y_pred)\n",
    "        mse = mean_squared_error(Y_test, Y_pred)\n",
    "        rmse = np.sqrt(mse)\n",
    "        results[name] = {'R2': r2, 'MSE': mse, 'RMSE': rmse}\n",
    "    print(\"Results from evaluate_models:\", results) \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"ChildProcessErrordef cross_validate_models(X, Y, models, cv=10):\n",
    "    results = {}\n",
    "    for name, model in models.items():\n",
    "        # Cross-validation for R2, MSE, RMSE\n",
    "        r2 = cross_val_score(model, X, Y, cv=cv, scoring='r2')\n",
    "        mse = cross_val_score(model, X, Y, cv=cv, scoring=make_scorer(mean_squared_error))\n",
    "        rmse = np.sqrt(mse)\n",
    "        \n",
    "        # Store average scores\n",
    "        results[name] = {\n",
    "            'mean_R2': np.mean(r2),\n",
    "            'mean_MSE': np.mean(mse),\n",
    "            'mean_RMSE': np.mean(rmse)\n",
    "        }\n",
    "    \n",
    "    return results\"\"\"\n",
    "\n",
    "def cross_validate_models_loo(X, Y, models):\n",
    "    results = {}\n",
    "    loo = LeaveOneOut()  # Leave-One-Out cross-validator\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        # Cross-validation for MSE, RMSE, and Explained Variance with LOO\n",
    "        mse = cross_val_score(model, X, Y, cv=loo, scoring=make_scorer(mean_squared_error))\n",
    "        rmse = np.sqrt(mse)  # RMSE is the square root of MSE\n",
    "        evs = cross_val_score(model, X, Y, cv=loo, scoring=make_scorer(explained_variance_score))  # Explained Variance\n",
    "        \n",
    "        # Store average scores\n",
    "        results[name] = {\n",
    "            'mean_MSE': np.mean(mse),\n",
    "            'mean_RMSE': np.mean(rmse),\n",
    "            'mean_Explained_Variance': np.mean(evs)  # Average explained variance\n",
    "        }\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_results_to_csv(results, filepath):\n",
    "    # Convert results to a DataFrame\n",
    "    results_df = pd.DataFrame.from_dict(results, orient='index')\n",
    "    \n",
    "    # Export DataFrame to CSV\n",
    "    results_df.to_csv(filepath, index=True)  # index=True to include model names in the CSV\n",
    "    print(f\"Results exported to {filepath}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(filepath, smiles_column,training_column,target_column):\n",
    "    # Step 1: Load dataset\n",
    "    df = load_dataset(filepath)\n",
    "    \n",
    "    # Step 2 & 3: Prepare the data (convert SMILES and separate X, Y)\n",
    "    X, Y = prepare_data(df, smiles_column,training_column, target_column)\n",
    "    \n",
    "    # Step 4: Split data into train and test sets\n",
    "    X_train, X_test, Y_train, Y_test = split_data(X, Y)\n",
    "    \n",
    "    # Step 5: Scale the data\n",
    "    X_train_scaled, X_test_scaled = scale_data(X_train, X_test)\n",
    "\n",
    "\n",
    "    \n",
    "    # Step 6: Evaluate models and print results\n",
    "    results = evaluate_models(X_train_scaled, X_test_scaled, Y_train, Y_test)\n",
    "    models = {\n",
    "        'Linear Regression': LinearRegression(),\n",
    "        'Ridge Regression': Ridge(alpha=1.0, solver='auto'),\n",
    "        'Lasso Regression': Lasso(alpha=0.01, max_iter=1000),\n",
    "        'Elastic Net': ElasticNet(alpha=0.01, l1_ratio=0.5, max_iter=1000),\n",
    "        'Random Forest': RandomForestRegressor(n_estimators=1000, max_depth=10, min_samples_split=2, random_state=42),\n",
    "        'SVR': SVR(kernel='rbf', C=1.0, epsilon=0.1),\n",
    "        'K-Neighbors Regressor': KNeighborsRegressor(n_neighbors=5, weights='uniform', algorithm='auto'),\n",
    "        'XGBoost': xgb.XGBRegressor(n_estimators=100, learning_rate=0.1, max_depth=6, subsample=0.8, colsample_bytree=0.8, random_state=42),\n",
    "        'CatBoost': CatBoostRegressor(learning_rate=0.1, iterations=1000, depth=6, silent=True),\n",
    "        'GradientBoost': GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, subsample=0.8, random_state=42)\n",
    "    }\n",
    "    \n",
    "    results1 = cross_validate_models_loo(X_train_scaled, Y_train, models)\n",
    "\n",
    "    for model_name, metrics in results1.items():\n",
    "        print(f\"Model: {model_name}, R2: {metrics['mean_Explained_Variance']:.4f}, MSE: {metrics['mean_MSE']:.4f}, RMSE: {metrics['mean_RMSE']:.4f}\")\n",
    " \n",
    "    #output_filepath = r\"C:\\Users\\matic\\OneDrive\\Desktop\\model_results_cross_maccs_1.csv\"\n",
    "    #export_results_to_csv(results1, output_filepath)\n",
    "    # Display model performances\n",
    "    for model_name, metrics in results.items():\n",
    "        print(f\"Model: {model_name}, R2: {metrics['R2']:.4f}, MSE: {metrics['MSE']:.4f}, RMSE: {metrics['RMSE']:.4f}\")\n",
    "\n",
    "    #output_filepath1 = r\"C:\\Users\\matic\\OneDrive\\Desktop\\model_results_maccs_1.csv\"\n",
    "    #export_results_to_csv(results, output_filepath1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    filepath = r\"C:\\Users\\matic\\OneDrive\\Desktop\\Konƒçne tabele t QSAR\\CLEAN CLEAN\\#testtes_clean.xlsx\"  # Path to your Excel file\n",
    "    smiles_column = \"SMILES\"  \n",
    "    target_column = \"E7\"    \n",
    "    training_column = \"Eq4-training\"\n",
    "    #Use 'Morgan', 'MACCS', 'AtomPair'\n",
    "    \n",
    "\n",
    "    main(filepath, smiles_column,training_column,target_column)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my-rdkit-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
